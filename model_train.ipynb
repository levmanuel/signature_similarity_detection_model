{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# signature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import itertools\n",
    "import pandas as pd\n",
    "from PIL import UnidentifiedImageError\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array, ImageDataGenerator\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Dense, Lambda, Dropout, BatchNormalization, Activation, GlobalAveragePooling2D\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value_counts des classes: label\n",
      "0    32411\n",
      "1     2834\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "root_dir = './data/genuine_signatures/'\n",
    "img_size = (128, 128)  # Taille standard pour les images\n",
    "data = []\n",
    "\n",
    "# Extensions d'images valides\n",
    "valid_extensions = ('.png', '.jpg', '.jpeg', '.bmp', '.tiff')\n",
    "\n",
    "# Parcours des sous-dossiers (chaque sous-dossier correspond à une personne)\n",
    "for person_dir in os.listdir(root_dir):\n",
    "    person_path = os.path.join(root_dir, person_dir)\n",
    "    \n",
    "    if os.path.isdir(person_path):\n",
    "        # Récupérer toutes les signatures dans le sous-dossier avec une extension valide\n",
    "        signatures = [s for s in os.listdir(person_path) if s.lower().endswith(valid_extensions)]\n",
    "        signatures_paths = [os.path.join(person_path, s) for s in signatures]\n",
    "        \n",
    "        # Créer des paires positives (intra-personne)\n",
    "        for pair in itertools.combinations(signatures_paths, 2):\n",
    "            data.append({\n",
    "                'image_1': pair[0],\n",
    "                'image_2': pair[1],\n",
    "                'label': 1  # Similaire (même personne)\n",
    "            })\n",
    "\n",
    "all_person_dirs = [os.path.join(root_dir, d) for d in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir, d))]\n",
    "\n",
    "for person_1, person_2 in itertools.combinations(all_person_dirs, 2):\n",
    "    signatures_person_1 = [s for s in os.listdir(person_1) if s.lower().endswith(valid_extensions)]\n",
    "    signatures_person_2 = [s for s in os.listdir(person_2) if s.lower().endswith(valid_extensions)]\n",
    "    \n",
    "    for sig1, sig2 in itertools.product(signatures_person_1, signatures_person_2):\n",
    "        data.append({\n",
    "            'image_1': os.path.join(person_1, sig1),\n",
    "            'image_2': os.path.join(person_2, sig2),\n",
    "            'label': 0  # Différent (personnes différentes)\n",
    "        })\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv('signature_pairs.csv', sep=\";\", index=False)\n",
    "\n",
    "def load_and_preprocess_image(img_path, target_size):\n",
    "    try:\n",
    "        img = load_img(img_path, target_size=target_size, color_mode='grayscale')\n",
    "        img = img_to_array(img) / 255.0  # Normalisation des pixels entre 0 et 1\n",
    "        return img\n",
    "    except UnidentifiedImageError:\n",
    "        print(f\"Erreur: Impossible de charger l'image {img_path}.\")\n",
    "        return None\n",
    "\n",
    "pairs = []\n",
    "labels = []\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    img1 = load_and_preprocess_image(row['image_1'], img_size)\n",
    "    img2 = load_and_preprocess_image(row['image_2'], img_size)\n",
    "    \n",
    "    if img1 is not None and img2 is not None:\n",
    "        pairs.append([img1, img2])\n",
    "        labels.append(row['label'])\n",
    "\n",
    "pairs = np.array(pairs)\n",
    "labels = np.array(labels)\n",
    "\n",
    "pairs_train, pairs_val, labels_train, labels_val = train_test_split(pairs, labels, test_size=0.2, random_state=42)\n",
    "print(f\"Value_counts des classes: {df['label'].value_counts()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de paires similaires (label 1): 2834\n",
      "Nombre de paires non similaires (label 0): 32411\n",
      "0    2834\n",
      "1    2834\n",
      "Name: count, dtype: int64\n",
      "Nombre total de paires après équilibrage: 5668\n",
      "Nombre de paires d'entraînement après équilibrage: 4534\n",
      "Nombre de paires de validation après équilibrage: 1134\n"
     ]
    }
   ],
   "source": [
    "num_similar = sum(labels == 1)\n",
    "num_different = sum(labels == 0)\n",
    "print(f\"Nombre de paires similaires (label 1): {num_similar}\")\n",
    "print(f\"Nombre de paires non similaires (label 0): {num_different}\")\n",
    "\n",
    "# Séparer les paires et labels en classes 0 et 1\n",
    "pairs_0 = pairs[labels == 0]\n",
    "labels_0 = labels[labels == 0]\n",
    "\n",
    "pairs_1 = pairs[labels == 1]\n",
    "labels_1 = labels[labels == 1]\n",
    "\n",
    "# Sous-échantillonnage des paires non similaires (label 0)\n",
    "pairs_0_downsampled, labels_0_downsampled = resample(pairs_0, labels_0,\n",
    "                                                     replace=False,    # Échantillonnage sans remplacement\n",
    "                                                     n_samples=len(labels_1),  # Même nombre que la classe minoritaire\n",
    "                                                     random_state=42)\n",
    "\n",
    "# Combinaison des deux classes après équilibrage\n",
    "pairs_balanced = np.vstack((pairs_0_downsampled, pairs_1))\n",
    "labels_balanced = np.hstack((labels_0_downsampled, labels_1))\n",
    "\n",
    "# Re-diviser en ensemble d'entraînement et de validation\n",
    "pairs_train, pairs_val, labels_train, labels_val = train_test_split(pairs_balanced, labels_balanced, test_size=0.2, random_state=42)\n",
    "\n",
    "# Vérifier la distribution après équilibrage\n",
    "print(pd.Series(labels_balanced).value_counts())\n",
    "\n",
    "print(f\"Nombre total de paires après équilibrage: {len(pairs_balanced)}\")\n",
    "print(f\"Nombre de paires d'entraînement après équilibrage: {len(pairs_train)}\")\n",
    "print(f\"Nombre de paires de validation après équilibrage: {len(pairs_val)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def contrastive_loss(y_true, y_pred, margin=1.0):\n",
    "    \"\"\"\n",
    "    y_true: labels (0 pour similaire, 1 pour dissemblable)\n",
    "    y_pred: distances prédites entre les paires d'images\n",
    "    margin: marge pour les paires dissemblables\n",
    "    \"\"\"\n",
    "    # Conversion de y_true en float32 pour éviter des erreurs de type\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    # Calcul des pertes pour les paires similaires et dissemblables\n",
    "    loss_similar = (1 - y_true) * 0.5 * K.square(y_pred)\n",
    "    loss_dissimilar = y_true * 0.5 * K.square(K.maximum(margin - y_pred, 0))\n",
    "    return K.mean(loss_similar + loss_dissimilar)\n",
    "\n",
    "def create_base_network(input_shape):\n",
    "    input = Input(shape=input_shape)\n",
    "    \n",
    "    # First Convolutional Block\n",
    "    x = Conv2D(64, (3, 3), padding='same', kernel_regularizer=l2(1e-4))(input)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv2D(64, (3, 3), padding='same', kernel_regularizer=l2(1e-4))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    \n",
    "    # Second Convolutional Block\n",
    "    x = Conv2D(128, (3, 3), padding='same', kernel_regularizer=l2(1e-4))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv2D(128, (3, 3), padding='same', kernel_regularizer=l2(1e-4))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    \n",
    "    # Third Convolutional Block\n",
    "    x = Conv2D(256, (3, 3), padding='same', kernel_regularizer=l2(1e-4))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv2D(256, (3, 3), padding='same', kernel_regularizer=l2(1e-4))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    \n",
    "    # Global Average Pooling\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    \n",
    "    # Fully Connected Layers\n",
    "    x = Dense(512, activation='relu', kernel_regularizer=l2(1e-4))(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(128, activation='relu', kernel_regularizer=l2(1e-4))(x)\n",
    "    \n",
    "    return Model(input, x)\n",
    "\n",
    "input_shape = (128, 128, 1)\n",
    "base_network = create_base_network(input_shape)\n",
    "\n",
    "input_a = Input(shape=input_shape)\n",
    "input_b = Input(shape=input_shape)\n",
    "\n",
    "processed_a = base_network(input_a)\n",
    "processed_b = base_network(input_b)\n",
    "\n",
    "def euclidean_distance(vects):\n",
    "    x, y = vects\n",
    "    sum_square = K.sum(K.square(x - y), axis=1, keepdims=True)\n",
    "    return K.sqrt(K.maximum(sum_square, K.epsilon()))\n",
    "\n",
    "distance = Lambda(euclidean_distance)([processed_a, processed_b])\n",
    "output = Dense(1, activation='sigmoid')(distance)\n",
    "model = Model([input_a, input_b], output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=0.0001)\n",
    "model.compile(loss=contrastive_loss, optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "141/141 [==============================] - 431s 3s/step - loss: 0.2634 - accuracy: 0.4958 - val_loss: 0.2365 - val_accuracy: 0.6852 - lr: 1.0000e-04\n",
      "Epoch 2/20\n",
      "141/141 [==============================] - 419s 3s/step - loss: 0.2233 - accuracy: 0.5033 - val_loss: 0.2196 - val_accuracy: 0.6675 - lr: 1.0000e-04\n",
      "Epoch 3/20\n",
      "141/141 [==============================] - 425s 3s/step - loss: 0.2088 - accuracy: 0.5573 - val_loss: 0.1894 - val_accuracy: 0.6208 - lr: 1.0000e-04\n",
      "Epoch 4/20\n",
      "141/141 [==============================] - 428s 3s/step - loss: 0.1967 - accuracy: 0.5869 - val_loss: 0.1827 - val_accuracy: 0.5547 - lr: 1.0000e-04\n",
      "Epoch 5/20\n",
      "141/141 [==============================] - 434s 3s/step - loss: 0.1893 - accuracy: 0.6144 - val_loss: 0.1725 - val_accuracy: 0.6481 - lr: 1.0000e-04\n",
      "Epoch 6/20\n",
      "141/141 [==============================] - 432s 3s/step - loss: 0.1820 - accuracy: 0.6202 - val_loss: 0.1672 - val_accuracy: 0.6817 - lr: 1.0000e-04\n",
      "Epoch 7/20\n",
      "141/141 [==============================] - 426s 3s/step - loss: 0.1762 - accuracy: 0.6337 - val_loss: 0.1684 - val_accuracy: 0.5944 - lr: 1.0000e-04\n",
      "Epoch 8/20\n",
      "141/141 [==============================] - 423s 3s/step - loss: 0.1700 - accuracy: 0.6524 - val_loss: 0.1547 - val_accuracy: 0.7240 - lr: 1.0000e-04\n",
      "Epoch 9/20\n",
      "141/141 [==============================] - 425s 3s/step - loss: 0.1646 - accuracy: 0.6615 - val_loss: 0.1533 - val_accuracy: 0.7390 - lr: 1.0000e-04\n",
      "Epoch 10/20\n",
      "141/141 [==============================] - 434s 3s/step - loss: 0.1604 - accuracy: 0.6641 - val_loss: 0.1513 - val_accuracy: 0.7425 - lr: 1.0000e-04\n",
      "Epoch 11/20\n",
      "141/141 [==============================] - 426s 3s/step - loss: 0.1558 - accuracy: 0.6708 - val_loss: 0.1475 - val_accuracy: 0.7249 - lr: 1.0000e-04\n",
      "Epoch 12/20\n",
      "141/141 [==============================] - 425s 3s/step - loss: 0.1518 - accuracy: 0.6780 - val_loss: 0.1395 - val_accuracy: 0.7672 - lr: 1.0000e-04\n",
      "Epoch 13/20\n",
      "141/141 [==============================] - 424s 3s/step - loss: 0.1483 - accuracy: 0.6775 - val_loss: 0.1402 - val_accuracy: 0.7399 - lr: 1.0000e-04\n",
      "Epoch 14/20\n",
      "141/141 [==============================] - 428s 3s/step - loss: 0.1454 - accuracy: 0.6801 - val_loss: 0.1337 - val_accuracy: 0.7734 - lr: 1.0000e-04\n",
      "Epoch 15/20\n",
      "141/141 [==============================] - 424s 3s/step - loss: 0.1410 - accuracy: 0.6964 - val_loss: 0.1323 - val_accuracy: 0.7628 - lr: 1.0000e-04\n",
      "Epoch 16/20\n",
      "141/141 [==============================] - 424s 3s/step - loss: 0.1389 - accuracy: 0.6901 - val_loss: 0.1309 - val_accuracy: 0.7769 - lr: 1.0000e-04\n",
      "Epoch 17/20\n",
      "141/141 [==============================] - 423s 3s/step - loss: 0.1354 - accuracy: 0.6997 - val_loss: 0.1247 - val_accuracy: 0.7831 - lr: 1.0000e-04\n",
      "Epoch 18/20\n",
      "141/141 [==============================] - 425s 3s/step - loss: 0.1331 - accuracy: 0.6977 - val_loss: 0.1264 - val_accuracy: 0.7593 - lr: 1.0000e-04\n",
      "Epoch 19/20\n",
      "141/141 [==============================] - ETA: 0s - loss: 0.1307 - accuracy: 0.7050\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "141/141 [==============================] - 426s 3s/step - loss: 0.1307 - accuracy: 0.7050 - val_loss: 0.1288 - val_accuracy: 0.7451 - lr: 1.0000e-04\n",
      "Epoch 20/20\n",
      "141/141 [==============================] - 424s 3s/step - loss: 0.1276 - accuracy: 0.7135 - val_loss: 0.1184 - val_accuracy: 0.7945 - lr: 5.0000e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x314a3cbd0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "model_checkpoint = ModelCheckpoint('models/best_model_V5.keras', save_best_only=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, min_lr= 0.00001, verbose=1)\n",
    "\n",
    "model.fit(\n",
    "    datagen.flow([pairs_train[:, 0], pairs_train[:, 1]], labels_train, batch_size=32),\n",
    "    steps_per_epoch=len(pairs_train) // 32,\n",
    "    epochs=20,\n",
    "    validation_data=([pairs_val[:, 0], pairs_val[:, 1]], labels_val),\n",
    "    callbacks=[early_stopping, model_checkpoint, reduce_lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36/36 [==============================] - 29s 812ms/step - loss: 0.1184 - accuracy: 0.7945\n"
     ]
    }
   ],
   "source": [
    "val_loss, val_accuracy = model.evaluate([pairs_val[:, 0], pairs_val[:, 1]], labels_val)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
